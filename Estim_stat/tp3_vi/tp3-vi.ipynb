{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1296a3d",
   "metadata": {},
   "source": [
    "# TP3 - *Latent Dirichlet Allocation* et Inférence variationnelle \n",
    "\n",
    "## Estimation avancée - G3 SDIA\n",
    "\n",
    "Dans ce TP, on s'intéresse à la méthode \"inférence variationnelle\" (VI) qui permet d'approcher la loi a posteriori d'un modèle (généralement inconnue) par une autre loi plus simple (généralement un produit de lois bien connues). Nous allons l'appliquer à un modèle probabiliste pour des données textuelles, appelé *Latent Dirichlet Allocation* (LDA, qui n'a rien à voir avec la LDA *Linear Discriminant Analysis* du cours de ML).\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Renommer votre notebook sous la forme `tp3_Nom1_Nom2.ipynb`, et inclure le nom du binôme dans le notebook. \n",
    "\n",
    "2. Votre code, ainsi que toute sortie du code, doivent être commentés !\n",
    "\n",
    "3. Déposer votre notebook sur Moodle dans la section prévue à cet effet avant la date limite : 23 Décembre 2023, 23h59."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d382bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle as pkl\n",
    "import scipy as ss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22423210",
   "metadata": {},
   "source": [
    "### Partie 0 - Introduction\n",
    "\n",
    "LDA is a popular probabilistic model for text data, introducted in [Blei et al. (2003)](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf). In this model, the posterior distribution is intractable, and we choose to resort to variational inference (note that a Gibbs sampler would be feasible as well, but would be very slow). In particular, the CAVI updates can be easily derived.\n",
    "\n",
    "In a few words, in LDA, each document is a mixture of topics, and each topic is a mixture of words. Uncovering those is the goal of *topic modeling*, and this is what we are going to do today. We will be using a collection of abstracts of papers published in JMLR (*Journal of Machine Learning Research*), one of the most prominent journals of the field.\n",
    "\n",
    "**Check the .pdf file describing the model.**\n",
    "The posterior is :\n",
    "$$p(\\boldsymbol{\\beta}, \\boldsymbol{\\theta}, \\mathbf{z} | \\mathcal{D}),$$\n",
    "which we are going to approximate in the following way :\n",
    "$$\\simeq \\left[ \\prod_{k=1}^K q(\\beta_k) \\right] \\left[ \\prod_{d=1}^D q(\\theta_d) \\right] \\left[ \\prod_{d=1}^D \\prod_{n=1}^{N_d} q(z_{dn}) \\right], $$\n",
    "with :\n",
    "* $q(\\beta_k)$ a Dirichlet distribution (of size V) with parameter $[\\lambda_{k1}, ...,\\lambda_{kV}]$\n",
    "* $q(\\gamma_d)$ a Dirichlet distribution (of size K) with parameter $[\\gamma_{d1}, ...,\\gamma_{dK}]$\n",
    "* $q(z_{dn})$ a Multinomial distribution (of size K) with parameter $[\\phi_{dn1}, ..., \\phi_{dnK}]$\n",
    "\n",
    "The updates are as follows :\n",
    "* $$\\lambda_{kv} = \\eta + \\sum_{d=1}^D \\sum_{n=1}^{N_d} w_{dnv} \\phi_{dnk} $$\n",
    "* $$\\gamma_{dk} = \\alpha + \\sum_{n=1}^{N_d} \\phi_{dnk}$$\n",
    "* $$ \\phi_{dnk} \\propto \\exp \\left( \\Psi(\\gamma_{dk}) + \\Psi(\\lambda_{k, w_{dn}}) - \\Psi(\\sum_{v=1}^V \\lambda_{kv}) \\right)$$\n",
    "\n",
    "$\\Psi$ is the digamma function, use `scipy.special.digamma`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059097f",
   "metadata": {},
   "source": [
    "### Partie 1 - Les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf0e5b",
   "metadata": {},
   "source": [
    "The data is already prepared, see code below. We have a total of 1898 abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41348f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "jmlr_papers = pkl.load(open(\"jmlr.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0694906",
   "metadata": {},
   "source": [
    "**Q1.** Fill in a list of keywords from the course, to see how many papers are about probabilistic ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321a5acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 467 Bayesian papers out of 1898\n"
     ]
    }
   ],
   "source": [
    "bayesian_jmlr_papers = []\n",
    "\n",
    "for paper in jmlr_papers:\n",
    "    bayesian_keywords = [\"prior\", \"posterior\", \"likelihood\", \"MAP\",\"Bayes\"]\n",
    "    if any([kwd in paper[\"abstract\"] for kwd in bayesian_keywords]):\n",
    "        bayesian_jmlr_papers.append(paper)\n",
    "        \n",
    "print(\"There are\", str(len(bayesian_jmlr_papers))+\" Bayesian papers out of\", str(len(jmlr_papers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a047cf9",
   "metadata": {},
   "source": [
    "Let us now preprocess the data. It is important to remove so-called \"stop-words\" like a, is, but, the, of, have... Scikit-learn will do the job for us. We will keep only the top-1000 words from the abstracts.\n",
    "\n",
    "As a result, we get the count matrix $\\mathbf{C}$ of size $D = 1898 \\times V = 1000$. $c_{dv}$ is the number of occurrences of word $v$ in document $d$. This compact representation is called \"bag-of-words\". Of course from $\\mathbf{C}$ you easily recover the words, since in LDA the order does not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "855d2359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100' '16' '17' '18' '949' '_blank' 'ability' 'able' 'abs' 'according'\n",
      " 'account' 'accuracy' 'accurate' 'achieve' 'achieved' 'achieves' 'action'\n",
      " 'actions' 'active' 'adaboost' 'adaptive' 'addition' 'additional'\n",
      " 'additive' 'address' 'advantage' 'advantages' 'agent' 'aggregation' 'al'\n",
      " 'algorithm' 'algorithmic' 'algorithms' 'allow' 'allowing' 'allows'\n",
      " 'alternative' 'analysis' 'analyze' 'applicable' 'application'\n",
      " 'applications' 'applied' 'apply' 'applying' 'approach' 'approaches'\n",
      " 'appropriate' 'approximate' 'approximately' 'approximation'\n",
      " 'approximations' 'arbitrary' 'art' 'article' 'artificial' 'associated'\n",
      " 'assume' 'assumed' 'assumption' 'assumptions' 'asymptotic'\n",
      " 'asymptotically' 'attributes' 'available' 'average' 'averaging' 'bandit'\n",
      " 'base' 'based' 'basic' 'basis' 'batch' 'bayes' 'bayesian' 'behavior'\n",
      " 'belief' 'benchmark' 'best' 'better' 'bias' 'bib' 'binary' 'block'\n",
      " 'boosting' 'bound' 'bounded' 'bounds' 'br' 'build' 'building' 'called'\n",
      " 'capture' 'carlo' 'case' 'cases' 'causal' 'central' 'certain' 'chain'\n",
      " 'challenge' 'challenging' 'characterization' 'characterize' 'choice'\n",
      " 'chosen' 'class' 'classes' 'classical' 'classification' 'classifier'\n",
      " 'classifiers' 'close' 'closed' 'cluster' 'clustering' 'clusters' 'code'\n",
      " 'coding' 'coefficient' 'coefficients' 'collection' 'color' 'com'\n",
      " 'combination' 'combine' 'combined' 'combining' 'common' 'commonly'\n",
      " 'community' 'comparable' 'compare' 'compared' 'comparing' 'comparison'\n",
      " 'comparisons' 'competitive' 'complete' 'complex' 'complexity' 'component'\n",
      " 'components' 'compression' 'computation' 'computational'\n",
      " 'computationally' 'compute' 'computed' 'computer' 'computing' 'concept'\n",
      " 'concepts' 'condition' 'conditional' 'conditions' 'confidence' 'consider'\n",
      " 'considered' 'consistency' 'consistent' 'consists' 'constant'\n",
      " 'constrained' 'constraint' 'constraints' 'construct' 'constructing'\n",
      " 'construction' 'context' 'continuous' 'contrast' 'contribution' 'control'\n",
      " 'converge' 'convergence' 'convex' 'core' 'correct' 'correlation'\n",
      " 'corresponding' 'cost' 'costs' 'covariance' 'criteria' 'criterion'\n",
      " 'cross' 'current' 'curve' 'dag' 'data' 'datasets' 'deal' 'decision'\n",
      " 'decomposition' 'deep' 'define' 'defined' 'definite' 'degree'\n",
      " 'demonstrate' 'demonstrated' 'density' 'dependence' 'dependencies'\n",
      " 'dependent' 'depends' 'derive' 'derived' 'descent' 'described'\n",
      " 'describes' 'design' 'designed' 'detection' 'determine' 'deterministic'\n",
      " 'develop' 'developed' 'dictionary' 'difference' 'differences' 'different'\n",
      " 'difficult' 'dimension' 'dimensional' 'dimensionality' 'dimensions'\n",
      " 'direct' 'directed' 'direction' 'directly' 'dirichlet' 'discovery'\n",
      " 'discrete' 'discriminant' 'discriminative' 'discuss' 'discussed'\n",
      " 'distance' 'distances' 'distributed' 'distribution' 'distributions'\n",
      " 'divergence' 'document' 'does' 'domain' 'domains' 'dual' 'dynamic'\n",
      " 'dynamics' 'easily' 'easy' 'edges' 'effect' 'effective' 'effectiveness'\n",
      " 'effects' 'efficiency' 'efficient' 'efficiently' 'elements' 'ell_1' 'em'\n",
      " 'embedding' 'empirical' 'empirically' 'enables' 'end' 'ensemble'\n",
      " 'entries' 'entropy' 'environment' 'epsilon' 'equivalence' 'equivalent'\n",
      " 'error' 'errors' 'especially' 'establish' 'established' 'estimate'\n",
      " 'estimated' 'estimates' 'estimating' 'estimation' 'estimator'\n",
      " 'estimators' 'et' 'euclidean' 'evaluate' 'evaluated' 'evaluation'\n",
      " 'evidence' 'exact' 'example' 'examples' 'exist' 'existence' 'existing'\n",
      " 'exists' 'expectation' 'expected' 'experimental' 'experiments' 'expert'\n",
      " 'experts' 'explicit' 'explicitly' 'exploit' 'exploiting' 'exploration'\n",
      " 'explore' 'exponential' 'expression' 'extend' 'extended' 'extension'\n",
      " 'extensions' 'extensive' 'fact' 'factor' 'factorization' 'factors'\n",
      " 'families' 'family' 'fast' 'faster' 'feature' 'features' 'field' 'fields'\n",
      " 'filtering' 'finally' 'finding' 'finite' 'fisher' 'fit' 'fitting' 'fixed'\n",
      " 'flexible' 'focus' 'following' 'font' 'form' 'formulation' 'formulations'\n",
      " 'framework' 'free' 'fully' 'function' 'functional' 'functions'\n",
      " 'fundamental' 'furthermore' 'future' 'games' 'gap' 'gaussian' 'gene'\n",
      " 'general' 'generalization' 'generalize' 'generalized' 'generally'\n",
      " 'generated' 'generating' 'generative' 'generic' 'geometric' 'geometry'\n",
      " 'github' 'given' 'gives' 'global' 'goal' 'good' 'gp' 'gradient' 'graph'\n",
      " 'graphical' 'graphs' 'gray' 'greedy' 'group' 'groups' 'guarantee'\n",
      " 'guarantees' 'hand' 'handle' 'hard' 'help' 'hidden' 'hierarchical' 'high'\n",
      " 'higher' 'highly' 'hilbert' 'hold' 'href' 'http' 'human' 'hypotheses'\n",
      " 'hypothesis' 'ica' 'idea' 'identify' 'identifying' 'ii' 'illustrate'\n",
      " 'image' 'images' 'implement' 'implementation' 'implementations'\n",
      " 'implemented' 'importance' 'important' 'improve' 'improved' 'improvement'\n",
      " 'improvements' 'improves' 'improving' 'include' 'includes' 'including'\n",
      " 'increasing' 'independence' 'independent' 'index' 'indicate' 'individual'\n",
      " 'induced' 'inference' 'infinite' 'influence' 'information' 'input'\n",
      " 'inputs' 'instance' 'instances' 'instead' 'interactions' 'interesting'\n",
      " 'interpretation' 'inthe' 'intractable' 'introduce' 'introduced'\n",
      " 'introduces' 'invariant' 'inverse' 'investigate' 'involves' 'involving'\n",
      " 'issue' 'issues' 'items' 'iteration' 'iterations' 'iterative' 'joint'\n",
      " 'kernel' 'kernels' 'key' 'knowledge' 'known' 'label' 'labeled' 'labels'\n",
      " 'lambda' 'language' 'large' 'larger' 'lasso' 'latent' 'layer' 'lead'\n",
      " 'leading' 'leads' 'learn' 'learned' 'learner' 'learners' 'learning'\n",
      " 'learns' 'level' 'li' 'library' 'like' 'likelihood' 'limit' 'limited'\n",
      " 'line' 'linear' 'linearly' 'literature' 'local' 'locally' 'log'\n",
      " 'logarithmic' 'logistic' 'long' 'loss' 'losses' 'low' 'lower' 'machine'\n",
      " 'machines' 'magnitude' 'main' 'make' 'makes' 'making' 'manifold' 'manner'\n",
      " 'map' 'margin' 'marginal' 'markov' 'match' 'matching' 'mathbb' 'mathcal'\n",
      " 'matlab' 'matrices' 'matrix' 'max' 'maximization' 'maximum' 'mcmc' 'mean'\n",
      " 'means' 'measure' 'measurements' 'measures' 'memory' 'message' 'method'\n",
      " 'methodology' 'methods' 'metric' 'minimal' 'minimax' 'minimization'\n",
      " 'minimize' 'minimizing' 'minimum' 'mining' 'missing' 'mixed' 'mixture'\n",
      " 'mixtures' 'model' 'modeling' 'models' 'monte' 'motivated' 'multi'\n",
      " 'multiclass' 'multiple' 'multivariate' 'mutual' 'naive' 'natural'\n",
      " 'naturally' 'nature' 'nbsp' 'near' 'nearest' 'necessary' 'need' 'needed'\n",
      " 'negative' 'neighbor' 'network' 'networks' 'neural' 'new' 'node' 'nodes'\n",
      " 'noise' 'noisy' 'non' 'nonlinear' 'nonparametric' 'norm' 'normal' 'norms'\n",
      " 'notion' 'novel' 'np' 'number' 'numbers' 'numerical' 'object' 'objective'\n",
      " 'objects' 'observation' 'observations' 'observed' 'obtain' 'obtained'\n",
      " 'obtaining' 'offers' 'ofthe' 'ones' 'online' 'onthe' 'open' 'operator'\n",
      " 'optimal' 'optimality' 'optimization' 'oracle' 'order' 'original'\n",
      " 'outperform' 'outperforms' 'output' 'outputs' 'overall' 'pac' 'package'\n",
      " 'pair' 'pairs' 'pairwise' 'paper' 'papers' 'parallel' 'parameter'\n",
      " 'parameters' 'parametric' 'partial' 'particular' 'particularly'\n",
      " 'partition' 'path' 'pattern' 'patterns' 'pca' 'pdf' 'penalized' 'penalty'\n",
      " 'perform' 'performance' 'performed' 'performs' 'perspective' 'phase'\n",
      " 'point' 'points' 'policies' 'policy' 'polynomial' 'popular' 'population'\n",
      " 'positive' 'possible' 'posterior' 'potential' 'power' 'powerful'\n",
      " 'practical' 'practice' 'precision' 'predict' 'predicting' 'prediction'\n",
      " 'predictions' 'predictive' 'predictor' 'predictors' 'presence' 'present'\n",
      " 'presented' 'presents' 'previous' 'previously' 'principal' 'principle'\n",
      " 'prior' 'priors' 'privacy' 'probabilistic' 'probabilities' 'probability'\n",
      " 'problem' 'problems' 'procedure' 'procedures' 'process' 'processes'\n",
      " 'processing' 'produce' 'product' 'program' 'programming' 'projection'\n",
      " 'propagation' 'properties' 'property' 'propose' 'proposed' 'provably'\n",
      " 'prove' 'provide' 'provided' 'provides' 'providing' 'purpose' 'python'\n",
      " 'quadratic' 'quality' 'queries' 'question' 'random' 'randomized' 'range'\n",
      " 'rank' 'ranking' 'rate' 'rates' 'real' 'recent' 'recently' 'recognition'\n",
      " 'recovery' 'reduce' 'reduced' 'reduces' 'reduction' 'regression' 'regret'\n",
      " 'regularization' 'regularized' 'reinforcement' 'related' 'relations'\n",
      " 'relationship' 'relationships' 'relative' 'relatively' 'relaxation'\n",
      " 'relevant' 'relies' 'rely' 'represent' 'representation' 'representations'\n",
      " 'represented' 'reproducing' 'require' 'required' 'requires' 'research'\n",
      " 'respect' 'response' 'restricted' 'result' 'resulting' 'results' 'reward'\n",
      " 'right' 'risk' 'rkhs' 'robust' 'robustness' 'role' 'rule' 'rules' 'run'\n",
      " 'running' 'sample' 'sampled' 'samples' 'sampling' 'scalable' 'scale'\n",
      " 'scales' 'scenarios' 'scheme' 'schemes' 'score' 'scoring' 'search'\n",
      " 'second' 'select' 'selection' 'semi' 'sense' 'sensitive' 'separation'\n",
      " 'sequence' 'sequences' 'sequential' 'series' 'set' 'sets' 'setting'\n",
      " 'settings' 'sgd' 'short' 'showing' 'shown' 'shows' 'showthat' 'signal'\n",
      " 'signals' 'significant' 'significantly' 'similar' 'similarity' 'simple'\n",
      " 'simulated' 'simulation' 'simulations' 'simultaneously' 'single'\n",
      " 'situations' 'size' 'sizes' 'small' 'smaller' 'smooth' 'social' 'soft'\n",
      " 'software' 'solution' 'solutions' 'solve' 'solved' 'solving' 'source'\n",
      " 'sources' 'space' 'spaces' 'sparse' 'sparsity' 'special' 'specific'\n",
      " 'specifically' 'specified' 'spectral' 'speed' 'squared' 'squares'\n",
      " 'stability' 'stable' 'stage' 'standard' 'state' 'stationary'\n",
      " 'statistical' 'statistics' 'step' 'steps' 'stochastic' 'strategies'\n",
      " 'strategy' 'strong' 'strongly' 'structural' 'structure' 'structured'\n",
      " 'structures' 'studied' 'studies' 'study' 'sub' 'subset' 'subsets'\n",
      " 'subspace' 'success' 'successfully' 'sufficient' 'suggest' 'suitable'\n",
      " 'sum' 'sup' 'supervised' 'support' 'surrogate' 'svm' 'svms' 'symmetric'\n",
      " 'synthetic' 'systems' 'table' 'takes' 'target' 'task' 'tasks' 'tdalign'\n",
      " 'technique' 'techniques' 'temporal' 'tensor' 'term' 'terms' 'test'\n",
      " 'testing' 'tests' 'text' 'thatthe' 'theorem' 'theoretic' 'theoretical'\n",
      " 'theoretically' 'theory' 'thispaper' 'threshold' 'tight' 'time' 'times'\n",
      " 'tool' 'toolbox' 'tools' 'topic' 'total' 'tr' 'tractable' 'trade'\n",
      " 'traditional' 'trained' 'training' 'transfer' 'treatment' 'tree' 'trees'\n",
      " 'true' 'type' 'types' 'typically' 'uncertainty' 'underlying'\n",
      " 'understanding' 'unified' 'uniform' 'universal' 'unknown' 'unlabeled'\n",
      " 'unlike' 'unsupervised' 'update' 'updates' 'upper' 'use' 'used' 'useful'\n",
      " 'usefulness' 'user' 'users' 'uses' 'using' 'usually' 'utility' 'v19'\n",
      " 'validation' 'value' 'valued' 'values' 'variable' 'variables' 'variance'\n",
      " 'variant' 'variants' 'variational' 'variety' 'various' 'varying' 'vector'\n",
      " 'vectors' 'version' 'versions' 'view' 'viewed' 'volume19' 'walk' 'way'\n",
      " 'weak' 'web' 'weight' 'weighted' 'weights' 'weshow' 'wide' 'widely'\n",
      " 'width' 'word' 'words' 'work' 'works' 'world' 'worst' 'years' 'yields'\n",
      " 'zero']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000, stop_words='english')\n",
    "X = vectorizer.fit_transform([paper[\"abstract\"] for paper in jmlr_papers])\n",
    "print(vectorizer.get_feature_names_out()) # Top-1000 words\n",
    "C = X.toarray() # Count matrix\n",
    "\n",
    "# Removing documents with 0 words\n",
    "idx = np.where(np.sum(C, axis = 1)==0)\n",
    "C = np.delete(C, idx, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e3267",
   "metadata": {},
   "source": [
    "**Q2.** How many elements of $\\mathbf{C}$ are non-zero ? Is this surprising ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9b9ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1895, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba38e64",
   "metadata": {},
   "source": [
    "Only 3 documents have been removed out of the 1898. It makes sense since these are theoretical papers which contain a lot of scientifique and specifique terms and we should have at least one of the 1000 most pertinent terms in each document. \n",
    "The vocabulary in this corpus of texts is very rich as such it's not suprising that there's more than 1000 words used in these documents which aren't stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72726d44",
   "metadata": {},
   "source": [
    "### Partie 2 - Inférence variationnelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada0b7f",
   "metadata": {},
   "source": [
    "As you know from the lecture, VI aims at maximizing the ELBO. I have prepared for you the function to compute the ELBO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb1f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import digamma, loggamma\n",
    "\n",
    "def ELBO(L, G, phi, a, e, W):\n",
    "    # Computes the ELBO with the values of the parameters L (Lambda), G (Gamma), and Phi\n",
    "    # a, e are hyperparameters (alpha and eta)\n",
    "    # W are the words (obsereved)\n",
    "    \n",
    "    # L - K x V matrix (variational parameters Lambda)\n",
    "    # G - D x K matrix (variational parameters Gamma)\n",
    "    # phi - List of D elements, each element is a Nd x K matrix (variational parameters Phi)\n",
    "    # a - Scalar > 0 (hyperparameter alpha)\n",
    "    # e - Scalar > 0 (hyperparameter eta)\n",
    "    # W - List of D elements, each element is a Nd x V matrix (observed words)\n",
    "    \n",
    "    e_log_B = (digamma(L).T - digamma(np.sum(L, axis = 1))).T\n",
    "    e_log_T = (digamma(G).T - digamma(np.sum(G, axis = 1))).T\n",
    "    \n",
    "    t1 = (e-1)*np.sum(e_log_B)\n",
    "    t2 = (a-1)*np.sum(e_log_T)\n",
    "\n",
    "    phi_s = np.zeros((D,K))\n",
    "    for d in range(0,D):\n",
    "        phi_s[d,:] = np.sum(phi[d], axis = 0)\n",
    "    t3 = np.sum(e_log_T*phi_s)\n",
    "    \n",
    "    tmp = np.zeros((K,V))\n",
    "    for d in range(0,D):\n",
    "        tmp = tmp + np.dot(phi[d].T, W[d])\n",
    "    t4 = np.sum(e_log_B*tmp)\n",
    "    \n",
    "    t5 = np.sum(loggamma(np.sum(L, axis = 1))) - np.sum(loggamma(L)) + np.sum((L-1)*e_log_B)\n",
    "    t6 = np.sum(loggamma(np.sum(G, axis = 1))) - np.sum(loggamma(G)) + np.sum((G-1)*e_log_T)\n",
    "\n",
    "    t7 = 0\n",
    "    for d in range(0,D):\n",
    "        t7 = t7 + np.sum(phi[d]*np.log(phi[d] + np.spacing(1)))\n",
    "\n",
    "    return t1 + t2 + t3 + t4 - t5 - t6 - t7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfc0302",
   "metadata": {},
   "source": [
    "**Q1.** Transform the matrix $\\mathbf{C}$ into the observed words $\\mathbf{w}$. $\\mathbf{w}$ should be a list of $D$ elements, each element of the list being a $N_d \\times V$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf794ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = []\n",
    "# On itére sur les documents \n",
    "for i in range(C.shape[0]):\n",
    "    # On initialise la matrice de taille Nd x V\n",
    "    doc = np.zeros(((C[i]).sum(),C.shape[1]))\n",
    "    # k compteur pour les lignes qui va aller de 1 à Nd\n",
    "    k = 0\n",
    "    for j in range(doc.shape[1]):\n",
    "        if C[i,j] != 0:\n",
    "            # Les C[i,j] lignes suivantes seront identiques avec un 1 en j et 0 sinon\n",
    "            for counts in range(C[i,j]):\n",
    "                doc[k+counts,j] = 1\n",
    "            k += C[i,j]\n",
    "    W.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0aa3d",
   "metadata": {},
   "source": [
    "**Q2.** Implement the CAVI algorithm. The updates are given at the beginning of the notebook. Monitor the convergence with the values of the ELBO (but start with a fixed number of iterations, like 50)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35664524",
   "metadata": {},
   "source": [
    "* $$\\lambda_{kv} = \\eta + \\sum_{d=1}^D \\sum_{n=1}^{N_d} w_{dnv} \\phi_{dnk} $$\n",
    "* $$\\gamma_{dk} = \\alpha + \\sum_{n=1}^{N_d} \\phi_{dnk}$$\n",
    "* $$ \\phi_{dnk} \\propto \\exp \\left( \\Psi(\\gamma_{dk}) + \\Psi(\\lambda_{k, w_{dn}}) - \\Psi(\\sum_{v=1}^V \\lambda_{kv}) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea905414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres pour les distributions Dirichlet\n",
    "D =  len(W)\n",
    "V = 1000\n",
    "K = 10 \n",
    "L = np.random.rand(K,V)\n",
    "G = np.ones((D,K))/K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c14607df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAVI(W, K, a, e, seed): # Other arguments may be added\n",
    "    # L - K x V matrix (variational parameters Lambda)\n",
    "    # G - D x K matrix (variational parameters Gamma)\n",
    "    # phi - List of D elements, each element is a Nd x K matrix (variational parameters Phi)\n",
    "    np.random.seed(seed)\n",
    "    V= 1000\n",
    "    D =len(W)\n",
    "    # Initiliase Lambda values\n",
    "    L = np.random.rand(K,V)\n",
    "    # Initialise Gamma values\n",
    "    G = np.ones((D,K))/K \n",
    "    # initialise phi\n",
    "    phi = []\n",
    "    for d in range(D):\n",
    "        Nd = len(W[d])\n",
    "        phi_d = np.zeros((Nd,K))\n",
    "        for n in range(Nd):\n",
    "            for k in range(K):\n",
    "                phi_d[n][k] = np.exp(ss.special.digamma(G[d][k])+ ss.special.digamma(L[k][np.where(W[d][n]==1)[0][0]]) - ss.special.digamma(np.sum(L[k])))\n",
    "        phi.append(phi_d)\n",
    "    elbo = ELBO(L, G, phi, a, e, W)\n",
    "    n_iter = 0\n",
    "    liste_elbo = [elbo]\n",
    "    while n_iter <20:\n",
    "        print(f'iteration: {n_iter}')\n",
    "        # Update the variables of q\n",
    "        L = np.full((K,V),e)\n",
    "        G = np.full((D,K),a)\n",
    "        # update L and G\n",
    "        for d in range(D):\n",
    "            L += phi[d][:][:].transpose()@W[d][:][:]\n",
    "            G[d] += phi[d].sum(axis=0)\n",
    "            \n",
    "        # update phi\n",
    "        for d in range(len(W)):\n",
    "            Nd = len(W[d])\n",
    "            for n in range(Nd):\n",
    "                for k in range(K):\n",
    "                    phi[d][n][k] = np.exp(ss.special.digamma(G[d][k])+ ss.special.digamma(L[k][np.where(W[d][n]==1)[0][0]]) - ss.special.digamma(np.sum(L[k])))\n",
    "\n",
    "        # compute the ELBO\n",
    "        res = ELBO(L, G, phi, a, e, W)\n",
    "        liste_elbo.append(res)\n",
    "        n_iter += 1\n",
    "    \n",
    "    return L, G, phi, liste_elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51ebf025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAVI_opti(W, K, a, e, seed, max_iter=20): # Other arguments may be added\n",
    "    # L - K x V matrix (variational parameters Lambda)\n",
    "    # G - D x K matrix (variational parameters Gamma)\n",
    "    # phi - List of D elements, each element is a Nd x K matrix (variational parameters Phi)\n",
    "    np.random.seed(seed)\n",
    "    V= 1000\n",
    "    D =len(W)\n",
    "    # Initiliase Lambda values\n",
    "    L = np.random.rand(K,V)\n",
    "    # Initialise Gamma values\n",
    "    G = np.ones((D,K))/K \n",
    "    # initialise phi\n",
    "    phi = []\n",
    "    for d in range(D):\n",
    "        Nd = len(W[d])\n",
    "        phi_d = np.zeros((Nd, K))\n",
    "        # Calcul des termes digamma pour G[d]\n",
    "        digamma_G = ss.special.digamma(G[d])\n",
    "\n",
    "        for n in range(Nd):\n",
    "            indices = np.where(W[d][n] != 0)[0]\n",
    "            phi_d[n] = np.exp(digamma_G + ss.special.digamma(L[:, indices[0]]) - ss.special.digamma(np.sum(L, axis=1)))\n",
    "            phi_d[n] =  phi_d[n]/np.sum(phi_d[n])\n",
    "        phi.append(phi_d)\n",
    "\n",
    "    elbo = ELBO(L, G, phi, a, e, W)\n",
    "    n_iter = 0\n",
    "    liste_elbo = np.zeros(max_iter)\n",
    "    liste_elbo[0] = elbo\n",
    "    \n",
    "    while n_iter <max_iter:\n",
    "        print(f'iteration: {n_iter}')\n",
    "        # Update the variables of q\n",
    "        L = np.full((K,V),e)\n",
    "        G = np.full((D,K),a)\n",
    "        for d in range(D):\n",
    "            L += phi[d][:][:].transpose()@W[d][:][:]\n",
    "            G[d] += np.sum(phi[d],axis=0)\n",
    "\n",
    "\n",
    "        for d in range(D):\n",
    "            Nd = len(W[d])\n",
    "            #phi_d = np.zeros((Nd, K))\n",
    "            \n",
    "            # Calcul des termes digamma pour G[d]\n",
    "            digamma_G = ss.special.digamma(G[d])\n",
    "\n",
    "            # Here the optimization \n",
    "            # Avoid one loop over K \n",
    "            \n",
    "\n",
    "            for n in range(Nd):\n",
    "                indices = np.where(W[d][n] != 0)[0]\n",
    "                phi[d][n] = np.exp(digamma_G + ss.special.digamma(L[:, indices[0]]) - ss.special.digamma(np.sum(L, axis=1)))\n",
    "                phi[d][n] =  phi[d][n]/np.sum(phi[d][n])\n",
    "\n",
    "        # compute\n",
    "        liste_elbo[n_iter] = ELBO(L, G, phi, a, e, W)\n",
    "        n_iter += 1\n",
    "    \n",
    "    return L, G, phi, liste_elbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88160d",
   "metadata": {},
   "source": [
    "**Q3.** Run the algorithm with $K = 10$, $\\alpha = 0.5$, $\\eta = 0.1$. From the results, compute the MMSE of $\\lambda_{kv}$ and $\\gamma_{dk}$.\n",
    "\n",
    "**Bonus** : Re-run the algorithm several times with different initializations, and keep the solution which returns the highest ELBO.\n",
    "\n",
    "NB : In my implementation, one iteration of the CAVI algorithm takes about 4 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee2cec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "iteration: 1\n",
      "iteration: 2\n",
      "iteration: 3\n",
      "iteration: 4\n",
      "iteration: 5\n",
      "iteration: 6\n",
      "iteration: 7\n",
      "iteration: 8\n",
      "iteration: 9\n",
      "iteration: 10\n",
      "iteration: 11\n",
      "iteration: 12\n",
      "iteration: 13\n",
      "iteration: 14\n",
      "iteration: 15\n",
      "iteration: 16\n",
      "iteration: 17\n",
      "iteration: 18\n",
      "iteration: 19\n",
      "iteration: 20\n",
      "iteration: 21\n",
      "iteration: 22\n",
      "iteration: 23\n",
      "iteration: 24\n",
      "iteration: 25\n",
      "iteration: 26\n",
      "iteration: 27\n",
      "iteration: 28\n",
      "iteration: 29\n",
      "iteration: 30\n",
      "iteration: 31\n",
      "iteration: 32\n",
      "iteration: 33\n",
      "iteration: 34\n",
      "iteration: 35\n",
      "iteration: 36\n",
      "iteration: 37\n",
      "iteration: 38\n",
      "iteration: 39\n",
      "iteration: 40\n",
      "iteration: 41\n",
      "iteration: 42\n",
      "iteration: 43\n",
      "iteration: 44\n",
      "iteration: 45\n",
      "iteration: 46\n",
      "iteration: 47\n",
      "iteration: 48\n",
      "iteration: 49\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "a = 0.5\n",
    "e = 0.1\n",
    "\n",
    "L, G, phi, list_ELBO = CAVI_opti(W, K, a, e, 123,max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "883be51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(50), list_ELBO)\n",
    "plt.title('Convergence de l\\'ELBO au fil des itérations')\n",
    "plt.xlabel('Nombre d\\'itérations')\n",
    "plt.ylabel('ELBO')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc6553",
   "metadata": {},
   "source": [
    "**Q4.** Based on the MMSE estimates :\n",
    "* What are the top-10 words per topic ? With your machine learning knowledge, can you make sense of some of the topics ?\n",
    "* Choose one document at random and display its topic proportions. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef772fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: model, time, networks, number, distribution, network, results, stochastic, models, parameters\n",
      "Topic 2: models, data, model, classification, task, tasks, machine, paper, used, multi\n",
      "Topic 3: algorithm, error, variables, method, problems, large, linear, function, bound, paper\n",
      "Topic 4: sup, problem, set, optimal, number, features, pdf, bib, small, decision\n",
      "Topic 5: kernel, em, training, data, dimensional, kernels, svm, space, based, high\n",
      "Topic 6: data, based, approach, framework, sets, real, method, feature, using, estimation\n",
      "Topic 7: sub, class, regression, loss, classification, bounds, selection, classes, risk, case\n",
      "Topic 8: nbsp, bayesian, inference, markov, href, gaussian, papers, font, 17, models\n",
      "Topic 9: learning, algorithms, algorithm, vector, function, methods, based, support, gradient, online\n",
      "Topic 10: analysis, problem, matrix, problems, new, optimization, convex, algorithms, low, methods\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Function to find top-10 words per topic\n",
    "def top_words_per_topic(L, vectorizer, n_words=10):\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "    top_words = []\n",
    "    for topic in range(L.shape[0]):\n",
    "        top_word_indices = np.argsort(L[topic])[::-1][:n_words]\n",
    "        top_words.append(feature_names[top_word_indices])\n",
    "    return top_words\n",
    "\n",
    "# Apply our function to the result of CAVI_opti \n",
    "top_words = top_words_per_topic(L, vectorizer)\n",
    "\n",
    "for i, words in enumerate(top_words):\n",
    "    print(f\"Topic {i+1}: {', '.join(words)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f9549f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic proportions for randomly chosen document (723):\n",
      "Topic 1: 0.6475\n",
      "Topic 2: 13.0466\n",
      "Topic 3: 9.1829\n",
      "Topic 4: 13.4794\n",
      "Topic 5: 13.8365\n",
      "Topic 6: 0.5942\n",
      "Topic 7: 3.7355\n",
      "Topic 8: 0.5709\n",
      "Topic 9: 0.6752\n",
      "Topic 10: 26.2315\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(31)\n",
    "# we choose a document \n",
    "random_doc_index = np.random.randint(len(G))\n",
    "doc_topic_proportions = G[random_doc_index]\n",
    "\n",
    "\n",
    "print(f\"Topic proportions for randomly chosen document ({random_doc_index + 1}):\")\n",
    "for topic, proportion in enumerate(doc_topic_proportions):\n",
    "    print(f\"Topic {topic + 1}: {proportion:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a18cd9",
   "metadata": {},
   "source": [
    "Topic 1 seems refer to neural network.\n",
    "\n",
    "Topic 2 seems refer to classification task.\n",
    "\n",
    "Topic 3 seems refer to ?.\n",
    "\n",
    "Topic 4 seems refer to ?.\n",
    "\n",
    "Topic 5 seems refer to SVM.\n",
    "\n",
    "Topic 6 seems refer to ?.\n",
    "\n",
    "Topic 7 seems refer to regression task.\n",
    "\n",
    "Topic 8 seems refer to bayesian inference.\n",
    "\n",
    "Topic 9 seems refer to ?.\n",
    "\n",
    "Topic 10 seems refer to optimization.\n",
    "\n",
    "For the document 723 the main topics seems to be 10, 5, 4, 3, 2. According to our previous guesses it could deal with classification with SVM methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cac1f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  We use convex relaxation techniques to provide a sequence ofregularized low-rank solutions for large-scale matrix completionproblems. Using the nuclear norm as a regularizer, we provide asimple and very efficient convex algorithm for minimizing thereconstruction error subject to a bound on the nuclear norm. Ouralgorithm SOFT-IMPUTE iteratively replaces the missingelements with those obtained from a soft-thresholded SVD. With warmstarts this allows us to efficiently compute an entireregularization path of solutions on a grid of values of theregularization parameter. The computationally intensive part of ouralgorithm is in computing a low-rank SVD of a dense matrix.Exploiting the problem structure, we show that the task can beperformed with a complexity of order linear in the matrix dimensions.  Oursemidefinite-programming algorithm is readily scalable to largematrices; for example SOFT-IMPUTE takes a few hours to compute low-rank approximationsof a <i>10<sup>6</sup></i> X <i>10<sup>6</sup></i> incomplete matrix with <i>10<sup>7</sup></i> observed entries, and fits a rank-<i>95</i> approximation to thefull Netflix training set in <i>3.3</i> hours.Our methods achieve good training and test errors and exhibit superior timings when compared to other competitive state-of-the-arttechniques.\n"
     ]
    }
   ],
   "source": [
    "print(jmlr_papers[722][\"abstract\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396a51c1",
   "metadata": {},
   "source": [
    "As we can see, it's still challenging to predict the topic of a random article.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446f419",
   "metadata": {},
   "source": [
    "----- Your answer here -----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13019da",
   "metadata": {},
   "source": [
    "**Q5.** Open questions :\n",
    "* What are some limitations of the LDA model ? Can you imagine an improvement ?\n",
    "* In this notebook, we have treated the hyperparameters as fixed. How could they be learned ?\n",
    "* Can you imagine a method to choose the number of topics ?\n",
    "* What strategies should we use to make the algorithm more efficient ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf69b12",
   "metadata": {},
   "source": [
    "We noticed in the top words list that some word irrevelant like \"using\" so it could be useful to delete more words than only stop words.\n",
    "Besides, here the number topic choice is arbitrary. It could be a good improvement to optimize this hyperparameter.\n",
    "Finally, we can debate on the hypothesis of the LDA considering each document is a mixture of topics, and each topic is a mixture of words. \n",
    "\n",
    "May be hyperpameters could be learned by cross validation.\n",
    "\n",
    "As for choosing the number of topics we could first use embedding models that capture semantic meaning like Word2Vec and use these vector embeddings to peform a clustering algorithm and choose the number of clusters/topics that minimises the intraclass distance for Kmeans or that minimises intraclass variance and maximise interclass variance for FDA for exemple. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600e6149",
   "metadata": {},
   "source": [
    "**BONUS.** Papier-crayon. À partir du modèle, pouvez-vous dériver les lois conditionnelles de l'échantillonneur de Gibbs ? Pour rappel, nous avons besoin de ces lois pour dériver ensuite les updates de l'algorithme CAVI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
