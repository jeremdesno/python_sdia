{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 : Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this work is to implement least square linear regression to medical data. The problem is based on an example described in the book by Hastie & Tibshirani (2009) pp. 3-4 & 49-63. Data come from a study published by Stamey et al. (1989). This study aims at the prediction of the level of prostate specific antigen, denoted by `lpsa` below, from the\n",
    "results of clinical exams. These exams are carried out before a possible\n",
    "prostatectomy.\n",
    "\n",
    "The measurements are log cancer volume `lcavol`, log prostate \n",
    "weight `lweight`, age of the patient `age`, log of benign prostatic \n",
    "hyperplasia amount `lbph`, seminal vesicle invasion `svi`, log of capsular \n",
    "penetration `lcp`, Gleason score `gleason`, and percent of Gleason scores 4 or \n",
    "5 `pgg45`. The variables `svi` and `gleason` are categorical, others are\n",
    "quantitative. There are `p=8` entries.\n",
    "The work is decomposed in the following tasks:\n",
    "\n",
    "* read and format the data : extraction of the training and test sets,\n",
    "* apply least square regression method to predict `lpsa` from the entries,\n",
    "* study the estimated error on the test set (validation),\n",
    "* identify the most significant entries by using a rejection test,\n",
    "* apply regularized least square regression method (ridge regression),\n",
    "* search for an optimal regularization parameter thanks to\n",
    "cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# import os\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Normalize data\n",
    "Data are stored in ASCII format: \n",
    "\n",
    "* the first column enumerates the data from 1 à 97 (97 male subjects). \n",
    "* columns 2 to 9 contain the entries themselves. \n",
    "* column 10 contains target values. \n",
    "* column 11 contains label 1 for the training set, \n",
    "and 2 for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% To read data from spaced separated float numbers\n",
    "# x, y = np.loadtxt(c, delimiter=',', usecols=(0, 2), unpack=True)\n",
    "\n",
    "data_init = np.loadtxt('prostate_data_sansheader.txt')\n",
    "\n",
    "data = data_init[:,1:]   # we get rid of the indices (1 to 97)\n",
    "\n",
    "#%% Extraction of training/test sets\n",
    "Itrain = np.nonzero(data[:,-1]==1)\n",
    "data_train=data[Itrain]   # original data\n",
    "\n",
    "Itest = np.nonzero(data[:,-1]==0)\n",
    "data_test = data[Itest]   # original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization of the data** *with respect to the mean and standard deviation of the training set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.23328245  0.47303067  7.44601122  1.45269103  0.41684299  1.39024269\n",
      "  0.70355366 29.08227243]\n"
     ]
    }
   ],
   "source": [
    "M_train = data_train\n",
    "M_test = data_test \n",
    "moy = np.zeros((8,))\n",
    "sigma = np.zeros((8,))\n",
    "\n",
    "# With a FOR loop :\n",
    "for k in range(8): # 8 columns of entries\n",
    "    moy[k]=np.mean(data_train[:,k])\n",
    "    sigma[k] = np.std(data_train[:,k], ddof=0)\n",
    "    M_train[:,k] = (data_train[:,k]-moy[k])/sigma[k] # normalized: centered, variance 1\n",
    "    M_test[:,k] = (data_test[:,k]-moy[k])/sigma[k]   # same normalization for test set\n",
    "\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Alternative WITHOUT FOR\\nnormalize = lambda vec: (vec-np.mean(vec))/np.std(vec)    # inline function \\nM_train = np.array( [ normalize(vec) for vec in data_train[:,0:8].T ] ).T  # iterate on vec direct / ARRAY not LIST\\nmoy = np.array( [ np.mean(vec) for vec in data_train[:,0:8].T ] )\\nsigma = np.array( [ np.std(vec, ddof=0) for vec in data_train[:,0:8].T ] )\\n\\nM_test = np.array([ (data_test[:,k]-moy[k])/sigma[k] for k in range(M_train.shape[1]) ] ).T\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Alternative WITHOUT FOR\n",
    "normalize = lambda vec: (vec-np.mean(vec))/np.std(vec)    # inline function \n",
    "M_train = np.array( [ normalize(vec) for vec in data_train[:,0:8].T ] ).T  # iterate on vec direct / ARRAY not LIST\n",
    "moy = np.array( [ np.mean(vec) for vec in data_train[:,0:8].T ] )\n",
    "sigma = np.array( [ np.std(vec, ddof=0) for vec in data_train[:,0:8].T ] )\n",
    "\n",
    "M_test = np.array([ (data_test[:,k]-moy[k])/sigma[k] for k in range(M_train.shape[1]) ] ).T\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : simple least square regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary questions\n",
    " \n",
    " * Compute the autocovariance matrix from the training set.\n",
    " * Observe carefully & Comment. What kind of information can you get ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lcavol</th>\n",
       "      <th>lweight</th>\n",
       "      <th>age</th>\n",
       "      <th>lbph</th>\n",
       "      <th>svi</th>\n",
       "      <th>lcp</th>\n",
       "      <th>gleason</th>\n",
       "      <th>pgg45</th>\n",
       "      <th>lpsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.535180</td>\n",
       "      <td>-1.810979</td>\n",
       "      <td>-1.980425</td>\n",
       "      <td>-1.003472</td>\n",
       "      <td>-0.537086</td>\n",
       "      <td>-0.843084</td>\n",
       "      <td>-1.039499</td>\n",
       "      <td>-0.903253</td>\n",
       "      <td>-0.430783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.871221</td>\n",
       "      <td>-0.647911</td>\n",
       "      <td>-0.906025</td>\n",
       "      <td>-1.003472</td>\n",
       "      <td>-0.537086</td>\n",
       "      <td>-0.843084</td>\n",
       "      <td>-1.039499</td>\n",
       "      <td>-0.903253</td>\n",
       "      <td>-0.162519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.479237</td>\n",
       "      <td>-1.976330</td>\n",
       "      <td>1.242777</td>\n",
       "      <td>-1.003472</td>\n",
       "      <td>-0.537086</td>\n",
       "      <td>-0.843084</td>\n",
       "      <td>0.381857</td>\n",
       "      <td>-0.215549</td>\n",
       "      <td>-0.162519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.041272</td>\n",
       "      <td>-0.725785</td>\n",
       "      <td>-0.906025</td>\n",
       "      <td>-1.003472</td>\n",
       "      <td>-0.537086</td>\n",
       "      <td>-0.843084</td>\n",
       "      <td>-1.039499</td>\n",
       "      <td>-0.903253</td>\n",
       "      <td>-0.162519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.455756</td>\n",
       "      <td>-0.409561</td>\n",
       "      <td>-0.368824</td>\n",
       "      <td>-1.003472</td>\n",
       "      <td>-0.537086</td>\n",
       "      <td>-0.843084</td>\n",
       "      <td>-1.039499</td>\n",
       "      <td>-0.903253</td>\n",
       "      <td>0.371564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lcavol   lweight       age      lbph       svi       lcp   gleason  \\\n",
       "0 -1.535180 -1.810979 -1.980425 -1.003472 -0.537086 -0.843084 -1.039499   \n",
       "1 -1.871221 -0.647911 -0.906025 -1.003472 -0.537086 -0.843084 -1.039499   \n",
       "2 -1.479237 -1.976330  1.242777 -1.003472 -0.537086 -0.843084  0.381857   \n",
       "3 -2.041272 -0.725785 -0.906025 -1.003472 -0.537086 -0.843084 -1.039499   \n",
       "4 -0.455756 -0.409561 -0.368824 -1.003472 -0.537086 -0.843084 -1.039499   \n",
       "\n",
       "      pgg45      lpsa  \n",
       "0 -0.903253 -0.430783  \n",
       "1 -0.903253 -0.162519  \n",
       "2 -0.215549 -0.162519  \n",
       "3 -0.903253 -0.162519  \n",
       "4 -0.903253  0.371564  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On charge les données dans un datframe + renommer les colonnes pour une meilleur visibilité \n",
    "df = pd.DataFrame(M_train[:,:-1])\n",
    "df = df.rename(columns={0:'lcavol', 1:'lweight', 2:'age', 3:'lbph', 4:'svi', 5:'lcp', 6:'gleason', 7:'pgg45', 8:'lpsa'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lcavol</th>\n",
       "      <th>lweight</th>\n",
       "      <th>age</th>\n",
       "      <th>lbph</th>\n",
       "      <th>svi</th>\n",
       "      <th>lcp</th>\n",
       "      <th>gleason</th>\n",
       "      <th>pgg45</th>\n",
       "      <th>lpsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.015</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.305</td>\n",
       "      <td>1.015</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.291</td>\n",
       "      <td>0.322</td>\n",
       "      <td>1.015</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.064</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.292</td>\n",
       "      <td>1.015</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.602</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>1.015</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.703</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.681</td>\n",
       "      <td>1.015</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.433</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.484</td>\n",
       "      <td>1.015</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.490</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.769</td>\n",
       "      <td>1.015</td>\n",
       "      <td>0.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.892</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.545</td>\n",
       "      <td>1.459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lcavol  lweight    age   lbph    svi    lcp  gleason  pgg45   lpsa\n",
       "0   1.015    0.305  0.291  0.064  0.602  0.703    0.433  0.490  0.892\n",
       "1   0.305    1.015  0.322  0.444  0.184  0.159    0.024  0.075  0.590\n",
       "2   0.291    0.322  1.015  0.292  0.131  0.176    0.371  0.280  0.277\n",
       "3   0.064    0.444  0.292  1.015 -0.141 -0.090    0.033 -0.031  0.320\n",
       "4   0.602    0.184  0.131 -0.141  1.015  0.681    0.312  0.489  0.678\n",
       "5   0.703    0.159  0.176 -0.090  0.681  1.015    0.484  0.673  0.595\n",
       "6   0.433    0.024  0.371  0.033  0.312  0.484    1.015  0.769  0.417\n",
       "7   0.490    0.075  0.280 -0.031  0.489  0.673    0.769  1.015  0.545\n",
       "8   0.892    0.590  0.277  0.320  0.678  0.595    0.417  0.545  1.459"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On affiche la covariance en prenant la transposé \n",
    "# car np.cov calcule la matrice de covariance basée sur les lignes et non les colonnes\n",
    "cov = np.round(pd.DataFrame(np.cov(df.transpose())),3).rename(columns={0:'lcavol', 1:'lweight', 2:'age', 3:'lbph', 4:'svi', 5:'lcp', 6:'gleason', 7:'pgg45', 8:'lpsa'})\n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Affichage sous forme d'une heatmap\n",
    "sns.set(font_scale=1.2)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cov, cmap='coolwarm', fmt='.2f', xticklabels=True, yticklabels=True)\n",
    "plt.title('Heatmap matrice de Covariance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "On observe déja que la matrice de covariance est bien symétrique. On retrouve les valeurs de variances et covariances des différentes features de notre matrice.\n",
    "On observe une variance de la target Lpsas plus forte (1,46) que la variance des autres colonnes qui est égale à 1,015 pour toute.\n",
    "On observe une covariance importante entre la target lpsas et les colonnes lcavol et svi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques rappels à propos de la notion de covariance : https://fr.wikipedia.org/wiki/Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 : least square regression \n",
    " * Build the matrix of features `X_train` for the training set, the first column is made of ones.\n",
    " * Estimate the regression vector `beta_hat` (estimates= `X*beta_hat`)\n",
    " _Indication: you may either use the function `inv` or another more efficient way to compute $A^{-1}B$ (think of `A\\B`)._ \n",
    " * What is the value of the first coefficient `beta_hat[0]` ? What does it correspond to ?\n",
    " * Estimate the prediction error (quadratic error) from the test set.\n",
    "\n",
    "\n",
    "*Indication: be careful of using `X_test` defined above, normalized w.r.t. the training data set. You can estimate this error by using:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test = data_test[:,8]   # target column\n",
    "N_test = data_test.shape[0]\n",
    "X_test = np.concatenate((np.ones((N_test,1)), M_test[:,0:8]), axis=1) \n",
    "# don't forget the 1st column of ones and normalization !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 9)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1\n",
    "#Building the matrix for the training set\n",
    "t_train = data_train[:,8]\n",
    "N_train = data_train.shape[0]\n",
    "X_train = np.concatenate((np.ones((N_train,1)), M_train[:, 0:8]), axis=1)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xdag shape: (9, 67)\n",
      "beta_hat shape: (9, 1)\n"
     ]
    }
   ],
   "source": [
    "#compute the Moore-Penrose pseudo inverse\n",
    "\n",
    "X_dag = la.inv(X_train.T @ X_train) @ X_train.T\n",
    "print(f\"Xdag shape: {X_dag.shape}\")\n",
    "\n",
    "#Compute beta_hat the regresion vector\n",
    "beta_hat = (X_dag @ t_train).reshape(-1, 1)\n",
    "print(f\"beta_hat shape: {beta_hat.shape}\")\n",
    "\n",
    "# Prediction on the test set\n",
    "t_hat = X_test @ beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 47.17329449881957\n"
     ]
    }
   ],
   "source": [
    "#Compute MSE\n",
    "epsilon =  (t_hat - t_test)**2\n",
    "MSE =  np.sum(epsilon)/ t_hat.shape[0]\n",
    "print(f\"MSE = {MSE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biais = 2.4523450850746267\n"
     ]
    }
   ],
   "source": [
    "#biais\n",
    "print(f\"biais = {beta_hat [0][0]}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beta_hat [0] corresponds to the biais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection test, computation of Z-scores\n",
    "Now we turn to the selection of the most significant entries so that our predictor be more robust. The essential idea is that our estimates will be more robust if only the most significant entries are taken into account. As a consequence, note that we will *reduce the dimension* of the problem from |p=8| to some smaller dimension. The present approach uses a statistical test to decide whether the regression coefficient corresponding to some entry is significantly non-zero. Then we can decide either to put non significant coefficients to zero, or to select the significant entries only and estimate the new reduced regression vector.\n",
    "\n",
    "Let's assume that target values are noisy due to some white Gaussian\n",
    "noise with variance $\\sigma^2$ (see Hastie & Tibshirani p. 47). One can show that the estimated regression vector |beta_hat| is also Gaussian with variance\n",
    "\n",
    "$$ var (\\widehat{\\beta}) = (X^TX)^{-1}\\sigma^2.$$  \n",
    "\n",
    "One can also show that the estimator of the variance (from the training set)\n",
    "\n",
    "$$\\widehat{\\sigma^2}=\\frac{1}{(N-p-1)}\\sum (t_n-\\widehat{t}_n)^2$$\n",
    "\n",
    "obeys a Chi-2 distribution. As a consequence a Chi-square statistical test can be used to determine whether some coefficient $\\beta_j$ is\n",
    "significantly non-zero. To this aim, one defines the variables $z_j$\n",
    "named Z-scores which in turn obey a Fisher law, also called\n",
    "$t$-distribution, which are often used in statistics:\n",
    "\n",
    "$$ z_j = \\frac{\\beta_j}{\\widehat{\\sigma}\\sqrt{v_j}} $$\n",
    "\n",
    "where $v_j$ is the $j$-th diagonal element of the matrix $(X^TX)^{-1}$.\n",
    "For sake of simplicity, we will consider that the null hypothesis of\n",
    "$\\beta_j$ is rejected with probability 95% if the Z-score is greater than 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "1. Compute the Z-scores and select the most significant entries.\n",
    "2. Estimate the prediction error over the test set if only these significant \n",
    "entries are taken into account for regression by putting other regression \n",
    "coefficients to zero.\n",
    "3. Estimate the new regression vector when only the significant features\n",
    "are taken into account.\n",
    "4. Compare to previous results (Exercise 1).\n",
    "\n",
    "*Indication 1 : to sort a vector `Z` in descending order*\n",
    "`val = np.sort(np.abs(Z))[-1:0:-1]`\n",
    "\n",
    "\n",
    "*Indication 2 :* to extract the diagonal of a matrix,\n",
    "`vXX = np.diag(inv(X.T.dot(X),k=0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 8\n",
      "\n",
      "vj shape: (9,)\n",
      "\n",
      "Sigma hat: 0.5073514562053173\n",
      "\n",
      "Z  = [39.565, 7.534, 3.862, -1.96, 2.886, 3.467, -2.621, -0.206, 2.44]\n",
      "['biais' 'lcavol' 'lweight' 'age' 'lbph' 'svi' 'lcp' 'gleason' 'pgg45']\n",
      "Selected entries: ['biais' 'lcavol' 'lweight' 'lbph' 'svi' 'lcp' 'pgg45']\n",
      "Entries where the null hypothesis wasn't rejected: ['age' 'gleason']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2\n",
    "#Define the variable number\n",
    "p = X_train.shape[1] - 1\n",
    "print(f\"p: {p}\\n\")\n",
    "\n",
    "#Extract the diagonal of a matrix\n",
    "vXX = np.diag(la.inv(X_train.T@ X_train))\n",
    "print(f\"vj shape: {vXX.shape}\\n\")\n",
    "\n",
    "# Compute an estimate of the variance sigma\n",
    "t_train_hat = X_train @ beta_hat\n",
    "sigma_hat = sum((t_train - t_train_hat.squeeze())**2)/(X_train.shape[0]-p-1)\n",
    "print(f\"Sigma hat: {sigma_hat}\\n\")\n",
    "\n",
    "# Compute Z_scores\n",
    "Z = [np.round(beta_hat[j][0]/(sigma_hat*np.sqrt(vXX[j])),3) for j in range (len(beta_hat))]\n",
    "print(f\"Z  = {Z}\")\n",
    "\n",
    "# Select the most significant entries with a 95% interval\n",
    "selected_features = np.abs(Z) > 2\n",
    "columns = np.array(['biais'] + df.columns.to_list()[:-1])\n",
    "print(columns)\n",
    "print(f\"Selected entries: {columns[selected_features]}\")\n",
    "# The following features are considered non significant\n",
    "print(f\"Entries where the null hypothesis wasn't rejected: {columns[ np.abs(Z) < 2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New values of beta hat: [[ 2.45234509]\n",
      " [ 0.71104059]\n",
      " [ 0.29045029]\n",
      " [ 0.        ]\n",
      " [ 0.21041951]\n",
      " [ 0.30730025]\n",
      " [-0.28684075]\n",
      " [ 0.        ]\n",
      " [ 0.27526843]]\n",
      "\n",
      "MSE: 50.03081886659442\n"
     ]
    }
   ],
   "source": [
    "beta_hat_new = beta_hat\n",
    "# We set to zero the values of regression coefficients that weren't conserved\n",
    "beta_hat_new[ np.abs(Z) < 2] = 0\n",
    "print(f\"New values of beta hat: {beta_hat_new}\\n\")\n",
    "\n",
    "# We compute an estimate of t_test with the new regression coefficients\n",
    "t_hat_new = X_test @ beta_hat_new\n",
    "# We compute the MSE\n",
    "MSE_new =  np.sum((t_hat_new - t_test)**2)/ t_test.shape[0]\n",
    "print(f\"MSE: {MSE_new}\")\n",
    "# The MSE_new > MSE so the model performs worse than before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xdag shape: (7, 67)\n",
      "beta_hat shape: (7, 1)\n",
      "MSE = 47.17329449881957\n"
     ]
    }
   ],
   "source": [
    "# Compute the regression estimate and MSE using only the selected entries\n",
    "\n",
    "#compute the Moore-Penrose pseudo inverse\n",
    "\n",
    "X_dag = la.inv(X_train[:,selected_features].T @ X_train[:,selected_features]) @ X_train[:,selected_features].T\n",
    "print(f\"Xdag shape: {X_dag.shape}\")\n",
    "\n",
    "#Compute beta_hat the regresion vector\n",
    "beta_hat_2 = (X_dag @ t_train).reshape(-1,1)\n",
    "print(f\"beta_hat shape: {beta_hat_2.shape}\")\n",
    "\n",
    "# Prediction on the test set\n",
    "t_hat = X_test[:,selected_features] @ beta_hat_2\n",
    "\n",
    "#Compute MSE\n",
    "epsilon =  (t_hat - t_test)**2\n",
    "MSE_2 =  np.sum(epsilon)/ t_hat.shape[0]\n",
    "print(f\"MSE = {MSE}\")\n",
    "\n",
    "# The MSE obtained by recomputing the regression vector estimate is identical to the one obtained previously \n",
    "# so we haven't improved the generalisation capability of our model but we've proved that the columns removed do not hold any influence on the prediction error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Regularized least squares\n",
    "This part deals with regularized least square regression. We denote\n",
    "by `beta_hat_reg` the resulting coefficients. This approach is an alternative to the selection based on statistical tests above. The idea is now to penalize large values of regression coefficients, *except for the bias*.\n",
    "\n",
    "We use the result:\n",
    "\n",
    "$$\\hat{\\beta} = (\\lambda I_p + X_c^T X_c)^{-1} X_c^T t_c$$\n",
    "\n",
    "where $X_c$ contains the normalized entries of the training data set with \n",
    "no column of ones (the bias should no be penalized and is processed). \n",
    "The targets `t_c` are therefore also centered, `t_c=t-mean(t)`.\n",
    " \n",
    "First, we estimate the bias $t_0$ to center the targets which yields the coefficient $\\beta_0$, that is `beta_hat_reg[0]` in Python.\n",
    "\n",
    "*Remark : the bias is estimated as the empirical average of targets.\n",
    "For tests, entries should be normalized with respect to the means and\n",
    "variances of the training data set (see exercise 3.5 p. 95 in Hastie & Tibshirani). Then work on the vector of entries with no column of ones.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "1. Use _ridge regression_ for penalty `lambda = 25` to estimate the regression vector. \n",
    "2. Estimate the prediction error from the test set.\n",
    "3. Compare the results (coefficients $\\beta$, error...) to previous ones.\n",
    "4. You may also compare these results to the result of best subset selection below:\n",
    "\n",
    "`beta_best = [2.477 0.74 0.316 0 0 0 0 0 0]`.\n",
    "\n",
    "*Indication : a simple way to obtain predictions for the test data set is the code below:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data_train[:,8]   # column of targets\n",
    "t0 = np.mean(t)\n",
    "\n",
    "N_test = data_test.shape[0]\n",
    "X_test = np.hstack((np.ones((N_test,1)), M_test[:,0:8]))  \n",
    "# Here the 1st column of X_test is a column of ones.\n",
    "#t_hat_reg = X_test.dot(beta_hat_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 9)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3\n",
    "#Building the matrix for the training set \n",
    "# We don't add the columns of one's as we do not want to penalise the bias\n",
    "t_c = data_train[:,8] - t0\n",
    "N_train = data_train.shape[0]\n",
    "X_c =  M_train[:, 0:8]\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_hat_reg shape: (9, 1)\n",
      "\n",
      "MSE_reg = 44.75465262184944\n",
      "\n",
      "MSE_2 = 47.66532790460011\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>beta_hat_reg</th>\n",
       "      <th>beta_hat_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>biais</td>\n",
       "      <td>2.452345</td>\n",
       "      <td>2.452345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lcavol</td>\n",
       "      <td>0.422109</td>\n",
       "      <td>0.678032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lweight</td>\n",
       "      <td>0.248792</td>\n",
       "      <td>0.266366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age</td>\n",
       "      <td>-0.042265</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lbph</td>\n",
       "      <td>0.165754</td>\n",
       "      <td>0.183008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>svi</td>\n",
       "      <td>0.230915</td>\n",
       "      <td>0.315281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lcp</td>\n",
       "      <td>0.010663</td>\n",
       "      <td>-0.265293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gleason</td>\n",
       "      <td>0.043060</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pgg45</td>\n",
       "      <td>0.131513</td>\n",
       "      <td>0.219316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  beta_hat_reg  beta_hat_2\n",
       "0    biais      2.452345    2.452345\n",
       "1   lcavol      0.422109    0.678032\n",
       "2  lweight      0.248792    0.266366\n",
       "3      age     -0.042265         NaN\n",
       "4     lbph      0.165754    0.183008\n",
       "5      svi      0.230915    0.315281\n",
       "6      lcp      0.010663   -0.265293\n",
       "7  gleason      0.043060         NaN\n",
       "8    pgg45      0.131513    0.219316"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute beta_hat the regresion vector\n",
    "ld = 25\n",
    "p = X_c.shape[1]\n",
    "beta_hat_reg = (la.inv(ld*np.eye(p) +  X_c.T @ X_c) @ X_c.transpose() @ t_c).reshape(-1, 1)\n",
    "\n",
    "# The biais is the mean of the training set target\n",
    "beta_0 = t0\n",
    "beta_hat_reg = np.concatenate((np.array(beta_0).reshape(1,1),beta_hat_reg))\n",
    "print(f\"beta_hat_reg shape: {beta_hat_reg.shape}\\n\")\n",
    "\n",
    "# Prediction on the test set\n",
    "t_hat = X_test @ beta_hat_reg\n",
    "\n",
    "#Compute MSE\n",
    "epsilon =  (t_hat - t_test)**2\n",
    "MSE_reg =  np.sum(epsilon)/ t_hat.shape[0]\n",
    "print(f\"MSE_reg = {MSE_reg}\\n\")\n",
    "print(f\"MSE_2 = {MSE_2}\\n\")\n",
    "\n",
    "# MSE_reg < MSE, using riddge regression we've managed to improve our MSE\n",
    "# Let's take a look at the coefficients \n",
    "pd.merge(pd.DataFrame({'index': columns, 'beta_hat_reg': beta_hat_reg.squeeze()}),(pd.DataFrame(({'index': columns[selected_features], 'beta_hat_2': beta_hat_2.squeeze()}))),how='outer', on= 'index')\n",
    "\n",
    "# We can observe that the biais stays the same as it's still t0. \n",
    "# Overall the coefficients of beta_hat_2 are bigger than the respective coefficients in beta_hat_reg, \n",
    "# which makes sense as ridge regression penalizes coefficients that are too high.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Cross-Validation \n",
    "\n",
    "## How to choose lambda from the training data set only ? \n",
    "\n",
    "The idea is to decompose the training set in 2 subsets: one subset for\n",
    "linear regression (say 9/10 of the data), the other to estimate the prediction error (say 1/10 of the data).\n",
    "\n",
    "We can repeat this operation 10 times over the 10 possible couples of\n",
    "subsets to estimate the average prediction error. We will choose the\n",
    "value of `lambda` which minimizes this error. The algorithm goes as\n",
    "follows:\n",
    "\n",
    "For the 10 cross-validation cases\n",
    "    \n",
    "    Extraction of test & training subsets `testset` & `trainset`\n",
    "    \n",
    "    For lambda in 0:40\n",
    "        Estimate `beta_hat` from normalized `trainset` (mean=0, var=1)\n",
    "        Estimate the error from  `testset`\n",
    "    EndFor lambda\n",
    "\n",
    "EndFor 10 cases\n",
    "\n",
    "Compute the average error for each lambda\n",
    "\n",
    "Choose `lambda` which minimizes the error \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "* Use 6-fold cross-validation in the present study to optimize the choice of `lambda`. \n",
    "Try values of `lambda` ranging from 0 to 40 for instance (0:40).\n",
    "* Plot the estimated error as a function of `lambda`.\n",
    "* Propose a well chosen value of `lambda` and give the estimated corresponding\n",
    "error on the test set.\n",
    "* Comment on your results.\n",
    "\n",
    "*Indication 1 : think of shuffling the dataset first.*\n",
    "\n",
    "*Indication 2 : you can build 6 training and test subsets by using the code below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmax = 40\n",
    "lambda_pos = arange(0,lmax+1) \n",
    "\n",
    "N_test = 10\n",
    "m=np.zeros(8)\n",
    "s = np.zeros(8)\n",
    "X_traink = np.zeros((X_train.shape[0]-N_test,8))\n",
    "X_testk = np.zeros((N_test,8))\n",
    "erreur = np.zeros((6,lmax+1))\n",
    "erreur_rel = np.zeros((6,lmax+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 \n",
    "random_seed = 10\n",
    "shuffled_data_train = data_train.copy()\n",
    "random.shuffle(shuffled_data_train)\n",
    "\n",
    "\n",
    "for ld in lambda_pos:\n",
    "    error_ld = np.zeros(6)\n",
    "    erreur_rel_ld = np.zeros(6)\n",
    "    for j in range(6):   # loop on test subsets\n",
    "        # extraction of testset\n",
    "        testset  = shuffled_data_train[arange(j*N_test,(j+1)*N_test),0:9] \n",
    "        # extraction of trainset\n",
    "        trainset = shuffled_data_train[hstack((arange(j*N_test),arange((j+1)*N_test,data_train.shape[0]))),0:9] \n",
    "        \n",
    "        # normalization of entries\n",
    "        normalize = lambda vec: (vec-np.mean(vec))/np.std(vec)   \n",
    "        moy = np.array( [ np.mean(vec) for vec in trainset[:,0:8].T ] )\n",
    "        sigma = np.array( [ np.std(vec, ddof=0) for vec in trainset[:,0:8].T ] )\n",
    "        X_c = np.array( [ normalize(vec) for vec in trainset[:,0:8].T ] ).T  # iterate on vec direct / ARRAY not LIST\n",
    "        X_test = np.array([ (testset[:,k]-moy[k])/sigma[k] for k in range(X_c.shape[1]) ] ).T\n",
    "        X_test  = np.hstack((np.ones((N_test,1)), X_test[:,0:8]))  \n",
    "\n",
    "        # We normalise the training target\n",
    "        t0 = np.mean(trainset[:,8])\n",
    "        t_c = trainset[:,8] - t0\n",
    "        t_test = testset[:,8]\n",
    "\n",
    "        # We compute the regression vector for the specifique fold and lambda value\n",
    "        p = X_c.shape[1]\n",
    "        beta_hat_reg = (la.inv(ld*np.eye(p) +  X_c.T @ X_c) @ X_c.transpose() @ t_c).reshape(-1, 1)\n",
    "\n",
    "        # The biais is the mean of the training set target\n",
    "        beta_0 = t0\n",
    "        beta_hat_reg = np.concatenate((np.array(beta_0).reshape(1,1),beta_hat_reg))\n",
    "        # Prediction on the test set\n",
    "        t_hat = X_test @ beta_hat_reg\n",
    "\n",
    "        #Compute MSE\n",
    "        epsilon =  (t_hat - t_test)**2\n",
    "        ecart_rel = [abs(t_hat[k][0] - t_test[k])/t_test[k] for k in range (len(t_hat))]\n",
    "        MSE_reg =  np.sum(epsilon)/ t_hat.shape[0]\n",
    "        error_ld[j] = MSE_reg\n",
    "        erreur_rel_ld[j] = np.mean(ecart_rel)\n",
    "\n",
    "    erreur[:,ld] = error_ld\n",
    "    erreur_rel[:,ld] = erreur_rel_ld\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.67850887, 14.52618012, 14.39127   , 14.26987953, 14.15934295,\n",
       "       14.0577545 , 13.96370193, 13.87610711, 13.79412623, 13.71708469,\n",
       "       13.6444333 , 13.57571789, 13.51055775, 13.44862989, 13.38965739,\n",
       "       13.33340062, 13.27965052, 13.22822329, 13.17895629, 13.13170472,\n",
       "       13.08633894, 13.04274234, 13.00080951, 12.96044478, 12.92156098,\n",
       "       12.8840784 , 12.84792391, 12.81303022, 12.77933523, 12.74678145,\n",
       "       12.71531558, 12.68488803, 12.65545262, 12.6269662 , 12.59938841,\n",
       "       12.57268143, 12.54680973, 12.52173993, 12.49744054, 12.47388191,\n",
       "       12.45103601])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "erreur.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0883559 , -0.11798818, -0.15089238, -0.18179922, -0.21096892,\n",
       "       -0.23860726, -0.26488104, -0.28992816, -0.31386449, -0.33678868,\n",
       "       -0.35870111, -0.37971343, -0.39993704, -0.41942938, -0.43788684,\n",
       "       -0.45573079, -0.47300453, -0.48974284, -0.50597728, -0.52173655,\n",
       "       -0.53704689, -0.55193232, -0.56628401, -0.58021733, -0.59379134,\n",
       "       -0.6070232 , -0.61992884, -0.63252306, -0.64481966, -0.65683153,\n",
       "       -0.66852392, -0.67982425, -0.69087619, -0.7016896 , -0.71227371,\n",
       "       -0.72263725, -0.73278844, -0.74273504, -0.7524844 , -0.76204347,\n",
       "       -0.77141884])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "erreur_rel.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 ---------------\n",
    "# ...\n",
    "# averaged error on the 6 training/test sets ?\n",
    "# averaged error on the 6 training/test sets ?\n",
    "# standard variation of this error estimate ?\n",
    "\n",
    "# print(erreur_lambda, std_erreur_lambda, erreur_rel_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 FIGURE ---------------\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 (continued)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
